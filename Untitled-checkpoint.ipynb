{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d32b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n",
      "403\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of countries\n",
    "countries = [\n",
    "    \"Indonesia\", \"United States\", \"Hongkong\", \"Germany\", \"United Kingdom\", \"Canada\", \"Japan\", \"China\",\n",
    "    \"France\", \"South Korea\", \"Taiwan\", \"Australia\", \"Netherlands\", \"Singapore\",\n",
    "    \"Thailand\", \"Malaysia\", \"Belgium\", \"Philippines\", \"Turkey\", \"Vietnam\", \"Portugal\"\n",
    "]\n",
    "\n",
    "# Function to scrape and save data for a given country\n",
    "def scrape_country_data(country):\n",
    "    # Construct the URL\n",
    "    url = f\"https://www.investing.com/equities/{country.lower().replace(' ', '-')}/top-stock-gainers\"\n",
    "\n",
    "    # Send an HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code)\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the div with the data-test attribute\n",
    "        dynamic_table_div = soup.find('div', {'data-test': 'dynamic-table'})\n",
    "\n",
    "        if dynamic_table_div:\n",
    "            # Find the table with the data inside the div\n",
    "            table = dynamic_table_div.find('table')\n",
    "\n",
    "            if table:\n",
    "                # Open a CSV file for writing\n",
    "                with open(f'{country}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    # Create a CSV writer\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "                    # Write header row\n",
    "                    csv_writer.writerow(['Name', 'Last', 'High', 'Low', 'Chg.', 'Chg.%', 'Vol.', 'Time'])\n",
    "\n",
    "                    # Find all rows in the table\n",
    "                    rows = table.find('tbody', {'class': 'datatable_body__tb4jX'}).find_all('tr')\n",
    "\n",
    "                    # Iterate over rows and write data to CSV\n",
    "                    for row in rows:\n",
    "                        # Extract data from each cell within the row\n",
    "                        cells = row.find_all('td')\n",
    "                        row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "                        # Check if the \"Chg.%\" is greater than +50.00%\n",
    "                        chg_percentage = row_data[5].replace('%', '')\n",
    "                        if chg_percentage and float(chg_percentage) > 30.00:\n",
    "                            # Write the row data to the CSV file\n",
    "                            csv_writer.writerow(row_data)\n",
    "\n",
    "# Scrape data for each country\n",
    "for country in countries:\n",
    "    scrape_country_data(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c92693d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b17da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to 'extracted_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the weather forecast URL\n",
    "url = \"http://www.values.com/inspirational-quotes\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all elements with the specified class\n",
    "target_elements = soup.find_all('div', class_='col-6 col-lg-4 text-center margin-30px-bottom sm-margin-30px-top')\n",
    "\n",
    "# Create a list to store the extracted data\n",
    "data_list = []\n",
    "\n",
    "# Extract and store href, src, Author, alt, and img information\n",
    "for element in target_elements:\n",
    "    a_tag = element.find('a')\n",
    "    img_tag = element.find('img')\n",
    "    h5_tag = element.find('h5', class_='value_on_red')\n",
    "\n",
    "    if a_tag and img_tag:\n",
    "        href = a_tag.get('href')\n",
    "        src = img_tag.get('src')\n",
    "        alt = img_tag.get('alt')\n",
    "        author = alt.split(\"#\")[1].strip() if \"#\" in alt else None\n",
    "        theme = h5_tag.text.strip() if h5_tag else None\n",
    "        \n",
    "        data_list.append({\n",
    "            'theme': theme,\n",
    "            'url': href,\n",
    "            'img': src,\n",
    "            'quote': alt,\n",
    "            'Author': author,\n",
    "            \n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('extracted_data.csv', index=False)\n",
    "\n",
    "print(\"Data extracted and saved to 'extracted_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef7907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
